\documentclass[a4paper, DIV12, headsepline]{scrartcl}

% common packages
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage[labelfont=bf]{caption}

% set head and foot
\usepackage{scrpage2}
\pagestyle{scrheadings}
\clearscrheadfoot
\ihead{Lab 6 -- Report}
\ohead{Group 25: Hui Jing (cid: huij), Tobias Fuchs (cid: fuchs)}
\cfoot{\pagemark}

% set pdf options
\usepackage[pdfborder={0 0 0}, bookmarksopen=true, bookmarksnumbered=true, pdftitle={Lab 6 Report}, pdfauthor={Hui Jing, Tobias Fuchs}, pdfsubject={Report}]{hyperref}

\begin{document}

\section*{Report for Lab 6}
\subsection*{Task 1 -- Basic Implementation}
\begin{itemize}
\item The code below shows our implementation:
\begin{verbatim}
    a = (float*)malloc(sizeof(float)* n * n); 
    b = (float*)malloc(sizeof(float)* n * n); 
    c = (float*)malloc(sizeof(float)* n * n); 
    d = (float*)malloc(sizeof(float)* n * n);
    ...
    /* Task: Memory Allocation */
    cudaMalloc((void **) &cuda_a, size);
    cudaMalloc((void **) &cuda_b, size);
    cudaMalloc((void **) &cuda_c, size);
    cudaMemset(&cuda_c, 0, size);

    /* Task: CUDA Memory Copy from Host to Device */
    cudaMemcpy(cuda_a, a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(cuda_b, b, size, cudaMemcpyHostToDevice);

    /* Task: Number of Blocks and Threads && Dimention*/
    dim3 dimGrid(100, 100, 1);
    dim3 dimBlock(10, 10, 1);

    // Kernel Execution
    matMultCUDA << < dimGrid, dimBlock >> >(cuda_a , cuda_b , cuda_c , n);

    /* Task: CUDA Memory Copy from Device to Host */
    cudaMemcpy(c, cuda_c, size, cudaMemcpyDeviceToHost);
\end{verbatim}

\item For each thread we calculate the global unique thread id and use it to determine which element in the output matrix the thread shall calculate. Kernel implementation is as follow:
\begin{verbatim}
__global__ static void matMultCUDA(const float* a, const float* b, float* c, int n)
{
    int threadId = (threadIdx.y + blockIdx.y * blockDim.y) * gridDim.x * blockDim.x +
                   (threadIdx.x + blockIdx.x * blockDim.x);
    int row = threadId/n;
    int col = threadId%n;
    int elementIdx = row * n + col;
    if(elementIdx < n*n) {
        for (int i = 0; i < n; i++) {
            c[elementIdx] += a[row * n + i] * b[i * n + col];
        }}
}
\end{verbatim}

\item We tried the following configurations(all Z-dimention settings are 1) in Table~\ref{tab:tab1Config} :
\begin{table}[htbp]
\centering
\begin{tabular}{cSSSS}
\hline
 & {Grid\_X} & {Grid\_Y} & {Block\_X} & {Block\_Y} \\
\hline
Config 1 & \SI{1000}{} & \SI{1000}{} & \SI{1}{} & \SI{1}{} \\
Config 2 & \SI{125}{} & \SI{125}{} & \SI{8}{} & \SI{8}{} \\
Config 3 & \SI{50}{} & \SI{50}{} & \SI{20}{} & \SI{20}{} \\
Config 4 & \SI{50}{} & \SI{25}{} & \SI{40}{} & \SI{20}{} \\
\hline
\end{tabular}
\caption{4 configurations}
\label{tab:tab1Config}
\end{table}

\item we used \texttt{nvcc -o matmul\_v1 cuda\_mm.cu; ./matmul\_v1} to compile and run the program.

\item The results is as shown in Table~\ref{tab:tab1ExecTime}. Since warp size is 32, thread block with fewer than 32 threads will not fully utilize the hardware. From the results, it is obvious that thread block with only a single thread is considerably worse than other configurations. Other configurations show similar performance in terms of execution time.
\begin{table}[htbp]
\centering
\begin{tabular}{cSSSS}
\hline
& {Config 1} & {Config 2} & {Config 3} & {Config 4} \\
\hline
Execution Time & \SI{3.860}{s} & \SI{3.168}{s} & \SI{3.183}{s} & \SI{3.144}{s} \\
\hline
\end{tabular}
\caption{Execution Time}
\label{tab:tab1ExecTime}
\end{table}

\end{itemize}

\subsection*{Task 2 -- Tiling Implementation using Shared Memory}
\begin{itemize}
\item The used machine has 49152 bytes shared memory per thread block.

\item Since we are using \texttt{floats} as a data type, each thread block can store 12288 \texttt{floats} in its shared memory. Therefore, each of the shared-memory matrices (\texttt{a}, \texttt{b} and \texttt{c}) can contain up to 4096 entries.

\item The following snippet shows the implementation of the new tiling matrix multiplication kernel.
\begin{verbatim}
__global__ static void matMultCUDA(const float *a, const float *b, float *c, int n)
{
    __shared__ float shared_a_tile[BLOCK_SIZE * BLOCK_SIZE];
    __shared__ float shared_b_tile[BLOCK_SIZE * BLOCK_SIZE];
    __shared__ float shared_c_tile[BLOCK_SIZE * BLOCK_SIZE];

    int j = threadIdx.x + blockIdx.x * BLOCK_SIZE;
    int i = threadIdx.y + blockIdx.y * BLOCK_SIZE;

    if (0 <= i && i < n && 0 <= j && j < n)
    {
    // Init shared output tile
    shared_c_tile[threadIdx.y * BLOCK_SIZE + threadIdx.x] = 0;

    // Iterate over all tiles
    for (int k_tile = 0; k_tile < n; k_tile += BLOCK_SIZE)
    {
        // Copy current tiles
        shared_a_tile[threadIdx.y * BLOCK_SIZE + threadIdx.x] = 
            a[i * n + (threadIdx.x + k_tile)];
        shared_b_tile[threadIdx.y * BLOCK_SIZE + threadIdx.x] = 
            b[(threadIdx.y + k_tile) * n + j];

        __syncthreads();

        // Compute tile of elements
        for (int k = k_tile; k < k_tile + BLOCK_SIZE && k < n; ++k)
        {
            shared_c_tile[threadIdx.y * BLOCK_SIZE + threadIdx.x] += 
                shared_a_tile[threadIdx.y * BLOCK_SIZE + (k % BLOCK_SIZE)] 
                * shared_b_tile[(k % BLOCK_SIZE) * BLOCK_SIZE + threadIdx.x];
        }

        __syncthreads();
    }

    c[i * n + j] = shared_c_tile[threadIdx.y * BLOCK_SIZE + threadIdx.x];
    }
}
\end{verbatim}

\item The \texttt{BLOCK\_SIZE} variable in the following snippet is instantiated with either 4, 8, 16 or 32. It is limited to 32 since $32 \cdot 32 = 1024$ is the maximum of threads per thread block. In the case of a block size of $4 \times 4$, for example, there are \texttt{ceil(n / BLOCK\_SIZE) = 250} blocks per dimension ($250 \times 250$ blocks in total).
\begin{verbatim}
// 2D config with given block size
dim3 dimGrid(ceil(n / BLOCK_SIZE), ceil(n / BLOCK_SIZE), 1);
dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE, 1);
\end{verbatim}

\item Table~\ref{tab:tab1ExecTiling} shows the execution times from 5 runs for all different configurations. Since the differences are not significant, there is no configuration that is better suited than the others. However, we expected that blocks of size $32 \times 32$ are best suited for the tiling matrix kernel since the larger the block size is, the less \texttt{\_\_syncthreads()} calls there are, which means less overhead.
\begin{table}[htbp]
\centering
\begin{tabular}{cSSSS}
\hline
 & {4} & {8} & {16} & {32} \\
\hline
Run 1 & \SI{3.252}{s} & \SI{3.189}{s} & \SI{3.323}{s} & \SI{3.085}{s} \\
Run 2 & \SI{3.138}{s} & \SI{3.183}{s} & \SI{3.285}{s} & \SI{3.355}{s} \\
Run 3 & \SI{3.127}{s} & \SI{3.115}{s} & \SI{3.111}{s} & \SI{3.317}{s} \\
Run 4 & \SI{3.123}{s} & \SI{3.288}{s} & \SI{3.097}{s} & \SI{3.100}{s} \\
Run 5 & \SI{3.140}{s} & \SI{3.139}{s} & \SI{3.094}{s} & \SI{3.104}{s} \\
\hline
Average & \SI{3.156}{s} & \SI{3.183}{s} & \SI{3.182}{s} & \SI{3.192}{s} \\
\hline
\end{tabular}
\caption{Execution times for different configurations.}
\label{tab:tab1ExecTiling}
\end{table}

\end{itemize}

\end{document}
