\documentclass[a4paper, DIV12, headsepline]{scrartcl}

% common packages
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage[labelfont=bf]{caption}

% set head and foot
\usepackage{scrpage2}
\pagestyle{scrheadings}
\clearscrheadfoot
\ihead{Lab 5 -- Report}
\ohead{Group 25: Hui Jing (cid: huij), Tobias Fuchs (cid: fuchs)}
\cfoot{\pagemark}

% set pdf options
\usepackage[pdfborder={0 0 0}, bookmarksopen=true, bookmarksnumbered=true, pdftitle={Lab 5 Report}, pdfauthor={Hui Jing, Tobias Fuchs}, pdfsubject={Report}]{hyperref}

\begin{document}

\section*{Report for Lab 5}
\subsection*{Task 1 -- MPI version}
\begin{itemize}
\item We followed the given instructions to build and run the code.

\item Text 2...

\item We distributed the weight vectors \texttt{W1}, \texttt{W2} and \texttt{W3} to allow the processes to work on a random portion of the samples on their own.

\item Text 4...

\item Every process computes the change of the weights, i.e, the variables \texttt{dWx}.

\item Text 6...

\item We used the given commands to figure out the number of total processes as well as the rank of the executing process.

\item Text 8...

\item We used the following strategy.
\begin{itemize}
\item Broadcast the randomly initialized weights to all processes.
\begin{verbatim}
// Random initialization of the weights in root process
vector<float> W1;
vector<float> W2;
vector<float> W3;

if (process_id == 0) {
    W1 = random_vector(784 * 128);
    W2 = random_vector(128 * 64);
    W3 = random_vector(64 * 10);
} else {
    W1.reserve(784 * 128);
    W2.reserve(128 * 64);
    W3.reserve(64 * 10);
}

// Broadcast initial weights
MPI_Bcast((void *)W1.data(), 784 * 128, MPI_FLOAT, 0, MPI_COMM_WORLD);
MPI_Bcast((void *)W2.data(), 128 * 64, MPI_FLOAT, 0, MPI_COMM_WORLD);
MPI_Bcast((void *)W3.data(), 64 * 10, MPI_FLOAT, 0, MPI_COMM_WORLD);
\end{verbatim}

\item When training the model, we use \verb|MPI_Allreduce| in order to sum over all weight changes and the original weights. After the reduction, the new weights values \texttt{W1}, \texttt{W2} and \texttt{W3} are present in all processes.
\begin{verbatim}
// Training loop
for (unsigned i = 0; i < 1000; ++i) {
   
    ... Calculate weight changes dW1, dW2, dW3 ...

    // Allreduce the weight deltas to all processes
    const auto dW1_aggr = process_id == 0 ? W1 - lr * dW1 : -lr * dW1;
    const auto dW2_aggr = process_id == 0 ? W2 - lr * dW2 : -lr * dW2;
    const auto dW3_aggr = process_id == 0 ? W3 - lr * dW3 : -lr * dW3;

    MPI_Allreduce((const void *)dW1_aggr.data(), (void *)W1.data(),
                  784 * 128, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);
    MPI_Allreduce((const void *)dW2_aggr.data(), (void *)W2.data(),
                  128 * 64, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);
    MPI_Allreduce((const void *)dW3_aggr.data(), (void *)W3.data(),
                  64 * 10, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);

    if ((process_id == 0) && (i + 1) % 100 == 0) {
        ... Logging ...
    }
}
\end{verbatim}
\end{itemize}

\item Text 10...

\end{itemize}

\subsection*{Task 2 -- Parallel GEMM per process}
Text...

\end{document}
